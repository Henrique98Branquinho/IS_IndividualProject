{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effef00a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75190c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc60a4a7",
   "metadata": {},
   "source": [
    "### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae0c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values Before Cleaning:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the data\n",
    "file = 'data.csv'\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# Step 2: Display the first few rows of the dataset\n",
    "#print(\"Original Data:\")\n",
    "#print(data.head())\n",
    "\n",
    "# Step 3: Check for missing values\n",
    "print(\"\\nMissing Values Before Cleaning:\")\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb5b54",
   "metadata": {},
   "source": [
    "### Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39d1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All columns are numerical.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check if all columns are numerical\n",
    "if all(data.dtypes.apply(lambda x: pd.api.types.is_numeric_dtype(x))):\n",
    "    print(\"\\nAll columns are numerical.\")\n",
    "else:\n",
    "    print(\"\\nNot all columns are numerical. Here's the breakdown:\")\n",
    "    print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354b3fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6ac912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in the target column:\n",
      "Bankrupt?\n",
      "0    6599\n",
      "1     220\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define y as the first column\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "# Check the balance of the binary target column\n",
    "print(\"Class distribution in the target column:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106f7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y as the first column and x as the rest\n",
    "y = data.iloc[:, 0]\n",
    "x = data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7586b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE:\n",
      "Bankrupt?\n",
      "1    6599\n",
      "0    6599\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "# Set the number of cores explicitly to avoid loky warnings\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Adjust the value to match your physical cores\n",
    "\n",
    "# Instantiate SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the dataset\n",
    "x_balanced, y_balanced = smote.fit_resample(x, y)\n",
    "\n",
    "# Output class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(y_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb4e2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Number of Features: 88\n",
      "Selected Features (Boolean Mask): [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True False False  True]\n",
      "Feature Ranking: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 7 1 1 1 4 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 6 1 1 1 1 3 8 1]\n",
      "Selected Feature Names: [' ROA(C) before interest and depreciation before interest', ' ROA(A) before interest and % after tax', ' ROA(B) before interest and depreciation after tax', ' Operating Gross Margin', ' Realized Sales Gross Margin', ' Operating Profit Rate', ' Pre-tax net Interest Rate', ' After-tax net Interest Rate', ' Non-industry income and expenditure/revenue', ' Continuous interest rate (after tax)', ' Operating Expense Rate', ' Research and development expense rate', ' Cash flow rate', ' Interest-bearing debt interest rate', ' Tax rate (A)', ' Net Value Per Share (B)', ' Net Value Per Share (A)', ' Net Value Per Share (C)', ' Persistent EPS in the Last Four Seasons', ' Cash Flow Per Share', ' Revenue Per Share (Yuan ¥)', ' Operating Profit Per Share (Yuan ¥)', ' Per Share Net profit before tax (Yuan ¥)', ' Realized Sales Gross Profit Growth Rate', ' Operating Profit Growth Rate', ' After-tax Net Profit Growth Rate', ' Regular Net Profit Growth Rate', ' Continuous Net Profit Growth Rate', ' Total Asset Growth Rate', ' Net Value Growth Rate', ' Cash Reinvestment %', ' Current Ratio', ' Quick Ratio', ' Total debt/Total net worth', ' Debt ratio %', ' Net worth/Assets', ' Long-term fund suitability ratio (A)', ' Borrowing dependency', ' Contingent liabilities/Net worth', ' Operating profit/Paid-in capital', ' Net profit before tax/Paid-in capital', ' Inventory and accounts receivable/Net value', ' Total Asset Turnover', ' Accounts Receivable Turnover', ' Average Collection Days', ' Inventory Turnover Rate (times)', ' Fixed Assets Turnover Frequency', ' Net Worth Turnover Rate (times)', ' Revenue per person', ' Operating profit per person', ' Allocation rate per person', ' Working Capital to Total Assets', ' Current Assets/Total Assets', ' Cash/Total Assets', ' Quick Assets/Current Liability', ' Cash/Current Liability', ' Current Liability to Assets', ' Operating Funds to Liability', ' Inventory/Current Liability', ' Current Liabilities/Liability', ' Working Capital/Equity', ' Current Liabilities/Equity', ' Long-term Liability to Current Assets', ' Retained Earnings to Total Assets', ' Total income/Total expense', ' Total expense/Assets', ' Current Asset Turnover Rate', ' Quick Asset Turnover Rate', ' Working capitcal Turnover Rate', ' Cash Turnover Rate', ' Cash Flow to Sales', ' Fixed Assets to Assets', ' Current Liability to Liability', ' Current Liability to Equity', ' Equity to Long-term Liability', ' Cash Flow to Total Assets', ' Cash Flow to Liability', ' CFO to Assets', ' Cash Flow to Equity', ' Current Liability to Current Assets', ' Liability-Assets Flag', ' Net Income to Total Assets', ' Total assets to GNP price', ' Gross Profit to Sales', \" Net Income to Stockholder's Equity\", ' Liability to Equity', ' Degree of Financial Leverage (DFL)', ' Equity to Liability']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Assuming x_balanced and y_balanced are your datasets\n",
    "# Convert to NumPy arrays for efficiency\n",
    "x_balanced_np = x_balanced.values if isinstance(x_balanced, pd.DataFrame) else x_balanced\n",
    "y_balanced_np = y_balanced.values if isinstance(y_balanced, pd.Series) else y_balanced\n",
    "\n",
    "# Step 1: Scale features for optimization and numerical stability\n",
    "scaler = StandardScaler()\n",
    "x_balanced_scaled = scaler.fit_transform(x_balanced_np)\n",
    "\n",
    "# Step 2: Initialize the Logistic Regression model\n",
    "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Step 3: Perform Recursive Feature Elimination with Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rfecv = RFECV(estimator=model, step=1, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "rfecv.fit(x_balanced_scaled, y_balanced_np)\n",
    "\n",
    "# Step 4: Extract and display the optimal number of features\n",
    "optimal_features = rfecv.n_features_\n",
    "selected_features = np.array(x_balanced.columns)[rfecv.support_]\n",
    "\n",
    "print(\"Optimal Number of Features:\", optimal_features)\n",
    "print(\"Selected Features (Boolean Mask):\", rfecv.support_)\n",
    "print(\"Feature Ranking:\", rfecv.ranking_)\n",
    "print(\"Selected Feature Names:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04750467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split and saved as train_data.csv and test_data.csv.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_balanced_scaled and y_balanced are prepared\n",
    "# Combine x and y into a single DataFrame for saving\n",
    "balanced_data = pd.DataFrame(x_balanced, columns=x.columns)\n",
    "balanced_data['target'] = y_balanced\n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "train_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=balanced_data['target'])\n",
    "\n",
    "# Save to CSV files\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(\"Data successfully split and saved as train_data.csv and test_data.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "IntelSis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
